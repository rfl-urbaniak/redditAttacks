---
layout: page
title: blah
output:
  github_document:
    pandoc_args:  --mathjax
  #  md_document:
#    variant: markdown_github

  preserve_yaml: true
bibliography: [../references/attacks.bib]
csl: [../references/apa-6th-edition.csl]
---

```{r , echo = FALSE, eval = TRUE}
knitr::opts_knit$set(base.dir = "/home/rafal/UG/UGprojects/redditAttacks",
base.url = "https://rfl-urbaniak.github.io/redditAttacks/")
knitr::opts_chunk$set(fig.path = "images/")
```




```{r setup, include=FALSE, echo=FALSE}
require("knitr")
#opts_knit$set(root.dir = "../")
options(knitr.kable.NA = '')

library(tidyverse)
library(knitr)
library(kableExtra)
library(formatR)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```







\noindent \textbf{Abstract.}
We conduct a large scale data-driven analysis of the effects of online personal attacks on social media user activity. First, we perform a thorough overview of the literature on the influence of social media on user behavior, especially on the impact that negative and aggressive behaviors, such as harassment and cyberbullying, have on users’ engagement in online media platforms. The majority of previous research were  small-scale self-reported studies, which is their limitation. This motivates our data-driven study. We perform a large-scale analysis of messages from Reddit, a discussion website, for a period of two weeks, involving 182,528 posts or comments to posts by 148,317 users. To efficiently collect and analyze the data we apply a high-precision personal attack detection technology. We analyze the obtained data from three perspectives: (i) classical statistical methods, (ii) Bayesian estimation, and (iii) model-theoretic analysis. The three perspectives agree: personal attacks decrease the victims’ activity.
The results can be interpreted as an important signal to social media platforms and policy makers that leaving personal attacks unmoderated is quite likely to disengage the users and in effect depopulate the platform. On the other hand, application of cyberviolence detection technology in combination with various mitigation techniques could improve and strengthen the user community. As more of our lives is taking place online, keeping the virtual space inclusive for all users becomes an important problem which online media platforms need to face.



Remark. In what follows, we sometimes display key pieces of code and explain what it does. Some not too fascinating pieces of code are supressed, but the reader can look them up in the associated .Rmd file and compile their own version.




#Technology applied for personal attack detection



For the need of this research we define personal attack as any kind of abusive remark made in relation to a person (ad hominem) rather than to the content of the argument expressed by that person in a discussion. The definition of ‘personal attack’ subsumes the use of specific terms which compare other people to animals or objects or making nasty insinuations without providing evidence. Three examples of typical personal attacks are as follows.


- *You are legit mentally retarded homie.*
- *Eat a bag of dicks, fuckstick.*
- *Fuck off with your sensitivity you douche.*


The detection of personal attacks was performed using  Samurai, a proprietary technology of Samurai Labs.^[[https://www.samurailabs.ai/](https://www.samurailabs.ai/), described in [@ptaszynski2018cyberbullying;@wroczynski2019system].]




The following figure illustrates how the input text ("ccant believ he sad ur an id10+...!") is processed step-by-step utilizing both statistical and symbolic methods.



\begin{figure}[h]
    \includegraphics[width=\linewidth]{../images/example-eps-converted-to.pdf}
    \caption{Example of processing of one sentence by the applied Samurai technology.}
    \label{fig:samuraiexample}
\end{figure}





In practice, it means that a whole variety of constructions can be detected without the need to construct a fixed list of dictionary words defined \textit{a priori}. Due to utilizing symbolic components that oversee statistical components, {\textsf Samurai} recognizes complex linguistic phenomena (such as indirect speech, rhetorical figures or counter-factual expressions) to distinguish personal attacks from normal communication, greatly reducing the number of false alarms as compared to others systems used for violence detection. An example of comparison can be seen in Figure \ref{fig:samuraiexample}, and a full benchmark was presented in [@ptaszynski2018cyberbullying].





The detection models utilized in this research were designed to detect personal attacks targeted against a second person (e.g. interlocutor, original author of a post) and a third person/group (e.g., other participants in the conversation, people not involved in the conversation, social groups, professional groups), except public figures (e.g. politicians, celebrities). 
With regards to symbolic component of the system, by "models" we mean separate rules (such as, specifying a candidate for the presence of personal attack, such as the aggressive word "idiot," which is further disambiguated with a syntactic rule of citation, e.g., "[he$\vert$she$\vert$they] said [SUBJECT] [PREDICATE]") or sets of rules, as seen in Figure \ref{fig:samuraiexample}, e.g. normalization model contains rules for transcription normalization, citation detection model contains rules for citation, etc. With regards to the statistical component, by "models" refer to machine learning models trained on large data to classify an entry into one of the categories (e.g., true personal attack, or false positive).



Moreover, the symbolic component of the system uses two types of symbolic rules, namely "narrow rules" and "wide rules." The former have smaller coverage (e.g., are triggered less often), but detect messages containing personal attacks with high precision.\footnote{Precision is defined traditionally as the ratio of correctly detected instances among all detected instances.} The latter, have wider coverage, but their precision is lower. 
We decided to set apart the "narrow" and "wide" subgroups of the detection models in order to increase the granularity of the analysis. Firstly, we took only the detection models designed to detect personal attacks targeted against second person. Secondly, we used these models on a dataset of 320,000 Reddit comments collected on 2019/05/06.
Thirdly, we randomly picked at most hundred returned results for each low-level model. Low-level models are responsible for detecting low-level categories. Similarly, mid-level models detect mid-level categories, by combining several low-level models, etc. (Some models are triggered very often while others rarely, so using all instances would create too much bias). There were 390 low-level models but many of them returned in less than 100 results. We verified them manually with the help of expert annotators trained in detection of personal attacks and selected only those models that achieved at least 90\% of precision. The models with fewer than 100 returned results were excluded from the selection. After this step, the "narrow" subgroup contained 43 out of 390 low-level models. Finally, we tested 
all of the "narrow" models on a large dataset of 477,851 Reddit comments collected between 2019/06/01 and 2019/08/31 from two subreddits (r/MensRights and r/TooAfraidToAsk). Each result of the "narrow" models was verified manually by a trained annotator and the "narrow" models collectively achieved over 93.3\% of precision. We also tested the rest of the "wide" models on random samples of 100 results for each model (from the previous dataset of 320,000 Reddit comments) and we excluded the models that achieved less than 80\% precision. The models with fewer than 100 results were not excluded from the "wide" group. In this simple setup we detected 24,251 texts containing "wide" attacks, where:

\begin{itemize}
\item  5,717 (23.6\%) contained personal attacks against second person detected by the "narrow" models,
\item  8,837 (36.4\%) contained personal attacks against second person detected by "wide" models
% the models other than "narrow",
\item  10,023 (41.3\%) contained personal attacks against third persons / groups.
The sum exceeds 100\% because some of the comments contained personal attacks against both second person and third person / groups. For example, a comment ``Fu$\ast\ast$ you a$\ast\ast$hole, you know that girls from this school are real bit$\ast\ast$es" contains both types of personal attack.
\end{itemize}

The sum exceeds 100\% because some of the comments contained personal attacks against both second person and third person / groups. For example, a comment ``Fu$\ast\ast$ you a$\ast\ast$hole, you know that girls from this school are real bit$\ast\ast$es" contains both types of personal attack.




Additionally, from the original data of 320,000 Reddit posts we extracted and annotated 6,769 Reddit posts as either a personal attack (1) or not (0). To assure that the extracted additional dataset contains a percentage of Personal Attacks sufficient to perform the evaluation, the Reddit posts were extracted with an assumption that each post contains at least one word of a general negative connotation. Our dictionary of such words contains 6,412 instances, and includes a wide range of negative words, such as nouns (e.g., "racist", "death", "idiot", "hell"), verbs (e.g., "attack", "kill", "destroy", "suck"), adjectives (e.g., "old", "thick", "stupid", "sick"), or adverbs (e.g., "spitefully", "tragically", "disgustingly"). In the 6,769 additionally annotated Reddit samples there were 957 actual Personal Attacks (14\%), from which Samurai correctly assigned 709 (true positives) and missed 248 (false negatives), which accounts for 74\% of the Recall rate.
Finally, we performed another additional experiment in which, we used Samurai to annotate completely new 10,000 samples from Discord messages that did not contain Personal Attacks but contained vulgar words. The messages were manually checked by two trained annotators and one additional super-annotator. The result of Samurai on this additional dataset was a 2\% of false positive rate, with exactly 202 cases misclassified as personal attacks. This accounts for specificity rate of 98\%.





\section{Study design, data collection, and dataset description}


The raw datasets used have been obtained by \textsf{Samurai Labs}, who were able to collect \textsf{Reddit} posts and comments without \textsf{Reddit} moderation or comment removal. All content was downloaded from the data stream provided by \url{pushshift.io} which enabled full data dump from Reddit in real-time. The advantage of using it was access to unmoderated data.\footnote{As of August 20th, the service is not available. For now, one possible way is to use an API provided by Reddit with a constraint:  Reddit allows for 600 requests every 10 minutes. 10 minutes might be a short time in the case of moderators' reaction, but is enough for AutoModerator or other automated moderation to exert their force, thus leading to a moderated, and therefore incomplete dataset.} Further, \textsf{Samurai Labs} deployed their personal attacks recognition algorithms to identify personal attacks.





In the study,   experimental manipulation of the crucial independent variables (personal attacks of various form)   to assess their effect on the dependent variable (users’ change in activity)  would be unethical and against the goal of Samurai Labs, which is to detect and _prevent_ online violence. While such a lack of control is a weakness as compared to typical experiments in psychology, our sample was both much larger and much more varied than the usual WEIRD (western, educated, and from industrialized, rich, and democratic countries) groups used in psychology. Notice, however, that the majority of Reddit users  are based in U.S. For instance,  [@wise2006moderation] examined  59 undergraduates from a political science class at a major Midwestern university in the USA, [@zong2019social] studied 251 students and faculty members from China who are users of WeChat, and  [@valkenburg2006friend] surveyed  881 young users (10-19yo.) of a Dutch SNS called CU2.



Because of the preponderance of personal attacks  online, we could use the real-life data from Reddit and use the following study design:




1. All the raw data, comprising of daily lists of posts and comments (some of which were used in the study) with time-stamps and author and target user names, have been obtained by  Samurai Labs, who also applied their personal attack detection algorithm to them, adding two more variables: narrow and wide. These were the raw datasets used in further analysis. 


2.  Practical limitations allowed for data collection for  around two continuous weeks (day 0 $\pm$ 7 days). First, we randomly selected one weekend day and one working day. These were June 27, 2020 (Saturday, S) and July 02, 2020 (Thursday, R). The activity on those days was used to   assign users to groups in the following manner. We picked one weekend and one non-weekend day to correct for activity shifts over the weekend (the data indeed revealed slightly higher activity over the weekends, no other week-day related pattern was observed). We could not investigate (or correct for) monthly activity variations, because the access to unmoderated data was limited.


3.  For each of these days, a random sample of 100,000 posts or comments have been drawn from all content posted on Reddit.  Each of these datasets went through preliminary user-name based bots removal. This is a simple search for typical phrases included in user names, such as "Auto", "auto", "Bot", or "bot".

For instance, for our initial thursdayClean datased, this proceeds like this:


\scriptsize
```{r,echo=TRUE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
thursdayClean <- thursdayClean[!grepl("Auto", thursdayClean$author, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("auto", thursdayClean$author, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("Auto", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("auto", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("bot", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("Bot", thursdayClean$receiver, fixed=TRUE),]
```
\normalsize




4. In some cases, content had been  deleted by the user or removed by Reddit  --- in such cases the dataset only contained information that some content had been  posted but was later removed; since we could not access the content  of such posts or comments and evaluate them for personal attacks, we also excluded them from the study.
 
 Again, this was a fairly straightforward use of grepl:

\scriptsize
```{r,echo=TRUE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
thursdayClean <- thursdayClean[!grepl("none", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("None", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("<MISSING>", thursdayClean$receiver, fixed=TRUE),]
thursdayClean <- thursdayClean[!grepl("[deleted]", thursdayClean$receiver, fixed=TRUE),]
```
\normalsize




5. This left us with  92,943 comments or posts by 75,516 users  for \textsf{R} and 89,585 comments by 72,801 users  for \textsf{S}. While we didn't directly track whether content was a post or a comment, we paid attention as to whether a piece of content was a reply to a post or not (the working assumption was that personal attacks on posts might have different impact than attacks on comments). Quite consistently, 46\% of content were comments on posts on both days. 



6.  On these two  days respectively,  1359 \textsf{R} users ($1.79\%$) received at least one \textsf{narrow} attack, 35 of them received more than one ($0.046\%$). 302 of \textsf{S} users ($0.39\%$) received at least one \textsf{narrow} attack and 3 of them more than one \textsf{narrow} on that day ($0.003\%$). These numbers  are estimates for a single day, and therefore if the chance of obtaining at least one \textsf{narrow} attack in a day is $1.79\%$, assuming the binomial distribution, the estimated probability of obtaining at least one \textsf{narrow} attack in a week is 11.9\% in a week and 43\% in a month.

\scriptsize
```{r,echo=TRUE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
100  * round(1-dbinom(0,7,prob = 1359/75516),3)` #week
100 * round(1-dbinom(0,31,prob = 1359/75516),3)` #month
```
\normalsize



7. To ensure a sufficient sample size, we decided not to draw a random sub-sample from the \textsf{wide > 1} or \textsf{narrow > 1} class comprising 340 users, and  included all of them in the Thursday treatment group (\textsf{Rtreatment}). Other users were randomly sampled from \textsf{wide > 0} and added to \textsf{Rtreatment}, so that the group count was 1000.

8. An analogous strategy was followed for \textsf{S}. 1338 users belonged to \textsf{wide > 0}, 27 to \textsf{wide > 1}, 329 to \textsf{narrow > 0} and 3 to \textsf{narrow > 1}. The total of 344 \textsf{wide > 1} or \textsf{narrow > 1} users was enriched with sampled \textsf{wide > 0} users to obtain the \textsf{Streatment} group of 1000 users. 

9. The preliminary \textsf{Rcontrol}/\textsf{Scontrol}  groups of 1500 users each were constructed by sampling 1500 users who posted comments on the respective days but did not receive any recognized attacks. The group sizes for control groups are higher, because after obtaining further information we intended to eliminate those who received any attacks  before the group selection day (and for practical reasons we could only  obtain data for this period after the groups were selected). 

10.  For each of these groups new  dataset was prepared, containing all posts or comments made by the users during the period of $\pm 7$ days from the selection day (337,015 for \textsf{Rtreatment}, 149,712 for \textsf{Rcontrol}, 227,980 for \textsf{Streatment} and 196,999 for \textsf{Scontrol}) and all comments made to their posts or comments (621,486 for \textsf{Rtreatment}, 170,422 for \textsf{Rcontrol}, 201,614 for \textsf{Streatment} and 204,456 for \textsf{Scontrol}), after checking for uniqueness these jointly were 951,949 comments for \textsf{Rtreatment}, 318,542 comments for \textsf{Rcontrol}, 404,535 comments for \textsf{Streatment}, and 380,692 comments for \textsf{Scontrol}). The need to collect all comments to the content posted by our group members was crucial. We needed this information because we needed to check all such comments for personal attacks to obtain an adequate count of attacks received by our group members. In fact, this turned out to be the most demanding part of data collection. 


11.  All these were wrangled into the frequency form, with (1) numbers of  attacks as recognized by  \textsf{narrow} or \textsf{wide} algorithm (in the dataset  we call these \textsf{high} and \textsf{low} respectively), (2) distinction between \textsf{attack on comment} and \textsf{attack on post}), and (3) activity counts for each day of the study, (4) with added joint counts for the \textsf{befor}e and \textsf{after} periods. Frequency data for users outside of the control or treatment groups were removed. 



12.  With the frequency form at hand, we could look at outliers. We used a fairly robust measure.\footnote{The classic outlier detection method takes a datapoint to be an outlier if it is at least two standard deviations from the mean. This method is not robust, because it is susceptible to the masking problem: adding an even more extreme outlier  to the data may easily make an outlier look normal. The boxplot rule that we used replaces standard deviation with the interquartile range which is much less sensitive to outliers.}  For each of the weekly counts of \textsf{low, high} and \textsf{activity} we calculated the interquartile range (\textsf{IQR}), as the absolute distance between the first and the third quartile and identified as outliers those users which landed at least $1.5\times \textsf{IQR}$ from the respective mean.  These resulted in a list of 534 "powerusers" which we suspected of being bots (even though we already removed users whose names suggested they were bots) --- all of them were manually checked by \textsf{Samurai Labs}. Those identified as bots (only 15 of them) or missing (29 of them) were removed. It was impossible to establish whether the missing users were bots; there are also two main reasons why a user might be missing: (a) account suspended, and (b) user deleted. We decided not to include the users who went missing in our study, because they would artificially increase the activity drop during the period and because we didn't suspect any of the user deletions to be caused by personal attacks directed against them (although someone might have deleted the account because they were attacked, these were power-users who have a high probability of having been attacked quite a few  times before, so this scenario is unlikely). 





13.  The frequency form of the control sets data was used to remove those users who were attacked in the \textsf{before} period (894 out of 1445 for \textsf{R}, and  982 out of 1447 for \textsf{S}   remained).




14.  A few more unusual data points needed to be removed, because they turned out to be users whose comments contained large numbers of third-person personal attacks which in fact supported them. Since we were interested in the impact of personal attacks directed against a user on the user's activity, such unusual cases would distort the results.  Six were authors of posts or comments which received more than 60 \textsf{wide only} attacks each. Upon inspection, all of them supported the original users. For instance,  two of them  were third-person comments about not wearing a mask or sneezing in public, not attacks on these users. Another example is  a female  who asked for advice about her husband: the comments were supportive of her and critical of the husband. Two users with weekly activity count change higher than 500 were removed -- they did not seem to be bots but very often they posted copy-pasted content and their activity patterns were highly irregular with changes most likely attributable to some other factors than attacks received. The same holds for a young user we removed from the study who displayed activity change near 1000. She   commented on her own activity during that period as very unusual and involving 50 hrs without sleeping. Her activity drop afterwards is very likely attributable to other factors than receiving a personal attack. 

15. 86 users who did not post anything in the \textsf{before} period were also removed.

16. In the end, \textsf{R} and \textsf{S} were aligned, centering around the selection day (day 8) and the studied group comprised 3673 users.


\footnotesize
```{r, echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
#note we load the data here
data <- read.csv("../datasets/quittingFinalAnon.csv")[,-1]
table(data$group)
```
\normalsize




A few first lines of the resulting anonymized dataset from which we removed separate day counts. Note that in the code "low" corresponds to "wide" (for "low precision") and "high" to "narrow" attacks (for "high precision"). The variables are: (low attacks, high attacks, low  attacks on posts, how  attacks on posts, authored content posted) and retained summary columns.




- \textsf{user} contains anonymous user numbers.
- \textsf{sumLowBefore} contains the sum of \textsf{wide}  attacks in days 1-7. \textsf{sumHighBefore} the sum of \textsf{narrow} (attacks in the same period. 
- \textsf{Pl} and \textsf{Ph} code \textsf{wide} and \textsf{narrow} attacks on posts (we wanted to verify the additional sub-hypothesis that  attacks on a post might have more impact than attacks on comments).

- \textsf{activityBefore} and \textsf{activityAfter} count comments or posts during days seven days before and seven days after. The intuition is, these shouldn't change much if personal attacks have  no impact on activity.

- \textsf{group} and \textsf{treatment} include information about which group a user belongs to.








```{r,,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
dataDisp <- data[,c(1,77:85)]
head(dataDisp)
```
\normalsize 






First, we visually explore our dataset by looking at the relationship between the number of received attacks vs. the activity change counted as the difference of weekly counts of posts or comments authored  in the second (\textsf{after}) and in the first  week (\textsf{before}). We do this for \textsf{narrow} attacks (Fig. \ref{fig:highPlots}), \textsf{wide} attacks (Fig. \ref{fig:lowPlots}), where a weaker, but still negative impact, can be observed, and  then we   take a look at the impact of those attacks which were recognized as \textsf{wide only} (Fig. \ref{fig:lowOnlyPlots}). The distinction between wide and narrow pertains only to the choice of attack recognition algorithm and does not directly translate into how offensive an attack was, except that \textsf{wide} attacks also include third-person ones.  Here, the direction of impact is less clear: while the tendency is negative for low numbers of attacks, non-linear smoothing suggests that  higher numbers mostly third-person personal attacks seem positively correlated with activity change. This might suggest that while being attacked has negative impact on a user's activity, having your post "supported" by other users' third-person attacks has a more motivating effect. We will look at this issue in a later section, when we analyze the dataset using regression.



The visualisations in Figure \ref{fig:highPlots} should be understood as follows. Each point is a user. The $x$-axis represents a number of attacks they received in the \textsf{before} period (so that, for instance, users with 0 wide attacks are the members of the control group), and  the $y$-axis represents the difference between their activity count \textsf{before} and \textsf{after}. We can see that most of the users received 0 attacks before (these are our control group members), with the rest of the group receiving 1, 2, 3, etc. attacks in the \textsf{before} period with decreasing frequency. The blue line represents linear regression suggesting negative correlation. The gray line is constructed using generalized additive mode (gam) smoothing, which is  a fairly standard smoothing method for large datasets (it is more sensitive to local tendencies and yet avoids overfitting). The parameters of the gam model (including the level of smoothing) are chosen by their predictive accuracy.\footnote{See  the documentation of \textsf{gam} of the \textsf{mgcv} packages for details: \url{https://www.rdocumentation.org/packages/mgcv/versions/1.8-33/topics/gam}.}  Shades indicate the 95\% confidence level interval for predictions from the linear model.   



\footnotesize

```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
library(ggthemes)
th <- theme_tufte()
highPlot <- ggplot(data, aes(x = sumHighBefore, y = activityDiff))+geom_jitter(size=0.8, alpha = 0.3)+
  geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", 
              size = 0.7, alpha = 0.8)+
  scale_x_continuous(breaks = 0:max(data$sumHighBefore),
  limits = c(-1,max(data$sumHighBefore)))+ylim(c(-300,300))+
  geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+   xlab("narrow attacks before")+ ylab("activity change after")+
labs(title = "Impact of narrow attacks on activity", subtitle = "weekly counts, n=3673")+
  geom_segment(aes(x = -1, y = -100, xend = 9, yend = -100), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ 
  geom_segment(aes(x = -1, y = 100, xend = 9, yend = 100), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+
  geom_segment(aes(x = -1, y = -100, xend = -1, yend = 100), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ 
  geom_segment(aes(x = 9, y = -100, xend = 9, yend = 100), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+th


highPlotZoomed <- 
ggplot(data, aes(x = sumHighBefore, y = activityDiff))+geom_jitter(size=1, alpha = 0.2)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th+scale_x_continuous(breaks = 0:max(data$sumHighBefore),limits = c(-1,9))+ylim(c(-100,100))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+ xlab("narrow attacks before")+ ylab("activity change after")+labs(title = "Impact of narrow attacks on activity", subtitle = "weekly counts, zoomed in")+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)
```
\normalsize



```{r highPlot,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
highPlot
```


```{r highPlotZoomed,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
highPlotZoomed
```





```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lowPlot <- ggplot(data, aes(x = sumLowBefore, y = activityDiff))+geom_jitter(size=0.8, alpha = 0.3)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+ xlab("wide attacks before")+ ylab("activity change after")+labs(title = "Impact of wide attacks on activity", subtitle = "weekly counts, n=3673") + geom_segment(aes(x = -1, y = -150, xend = 15, yend = -150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = -1, y = 150, xend = 15, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = -1, y = -150, xend = -1, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = 15, y = -150, xend = 15, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+xlim(c(-1,max(data$sumLowBefore)))



lowPlotZoomed <- ggplot(data, aes(x = sumLowBefore, y = activityDiff))+geom_jitter(size=1, alpha = 0.2)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th+scale_x_continuous(breaks = 0:max(data$sumLowBefore),limits = c(-1,15))+ylim(c(-150,150))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+ xlab("wide attacks before")+ ylab("activity change after")+labs(title = "Impact of wide attacks on activity", subtitle = "weekly counts, zoomed in")+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)
```


```{r lowPlot,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowPlot
```



```{r lowPlotZoomed,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowPlotZoomed
```











```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lowOnlyPlot <- ggplot(data, aes(x = (sumLowBefore - sumHighBefore), y = activityDiff))+geom_jitter(size=0.8, alpha = 0.3)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+ xlab("wide only attacks before")+ ylab("activity change after")+labs(title = "Impact of wide only attacks on activity", subtitle = "weekly counts, n=3673") + geom_segment(aes(x = -1, y = -150, xend = 15, yend = -150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = -1, y = 150, xend = 15, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = -1, y = -150, xend = -1, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+ geom_segment(aes(x = 15, y = -150, xend = 15, yend = 150), lty = 3, size = 0.1, color = "gray71", alpha = 0.2)+xlim(c(-1,max(data$sumLowBefore)))


lowOnlyPlotZoomed <- ggplot(data, aes(x = (sumLowBefore - sumHighBefore), y = activityDiff))+geom_jitter(size=1, alpha = 0.2)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th+scale_x_continuous(breaks = 0:max(data$sumLowBefore),limits = c(-1,15))+ylim(c(-150,150))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)+ xlab("wide only attacks before")+ ylab("activity change after")+labs(title = "Impact of wide only attacks on activity", subtitle = "weekly counts, zoomed in")+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)
```




```{r lowOnlyPlot,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowOnlyPlot
```


```{r lowOnlyPlotZoomed,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowOnlyPlotZoomed
```





```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
rescale <- function(diff,act){
diff/act
}
data$activityScore <- rescale(data$activityDiff,data$activityBefore)

propPlotHigh <- ggplot(data, aes(x = sumHighBefore, y = activityScore))+geom_jitter(size=0.8, alpha = 0.3)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th + xlab("narrow attacks before")+ ylab("proportional activity change")+labs(title = "Impact of narrow attacks on proportional activity", subtitle = "n=3673")+scale_y_continuous(limits = c(-1,10))+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)+scale_x_continuous(breaks = 1:15, limits = c(-1,10))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)

propPlotLow <- ggplot(data, aes(x = sumLowBefore, y = activityScore))+geom_jitter(size=0.8, alpha = 0.3)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th + xlab("wide attacks before")+ ylab("proportional activity change")+labs(title = "Impact of wide attacks on proportional activity", subtitle = "n=3673")+scale_y_continuous(limits = c(-1,10))+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)+scale_x_continuous(breaks = 1:15, limits = c(-1,10))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)


propPlotLowOnly <- ggplot(data, aes(x = sumLowBefore-sumHighBefore, y = activityScore))+geom_jitter(size=0.8, alpha = 0.3)+geom_smooth(method = "lm",color = "skyblue", fill= "skyblue", size = 0.7, alpha = 0.8)+th + xlab("wide only attacks before")+ ylab("proportional activity change")+labs(title = "Impact of wide only attacks on proportional activity", subtitle = "n=3673")+scale_y_continuous(limits = c(-1,10))+geom_hline(yintercept = 0, col = "red", size = 0.2, lty = 3)+scale_x_continuous(breaks = 1:15, limits = c(-1,10))+geom_smooth(color = "grey", size = 0.4, lty = 2, alpha = 0.2)

```


```{r propPlotHigh,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
propPlotHigh
```





```{r propPlotLow,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
propPlotLow
```




```{r propPlotLowOnly,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
propPlotLowOnly
```




```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",message= FALSE, warning= FALSE}
counts <- t(table(data$sumHighBefore)) 
counts <- rbind(counts,counts)
counts[1,] <- as.numeric(colnames(counts))
colnames(counts) <- NULL
rownames(counts) <-  c("no. of attacks", "count")
counts
```




```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",message= FALSE, warning= FALSE}
attacks <- 0:8 
max <- max(attacks)
low <- numeric(max+1)
high <- numeric(max+1)
m <- numeric(max+1)
p <- numeric(max+1)
t <- list()

for(attacks in attacks){
t[[attacks+1]] <- t.test(data[data$sumHighBefore == attacks,]$activityDiff)

low[attacks+1] <- t[[attacks+1]]$conf.int[1]
high[attacks+1] <- t[[attacks+1]]$conf.int[2]
m[attacks+1] <- t[[attacks+1]]$estimate
p[attacks+1] <- t[[attacks+1]]$p.value
}
highTable<-as.data.frame(round(rbind(0:8,low, m, high, p),3))
rownames(highTable) <- c("attacks","CIlow","estimated m", "CIhigh", "p-value")
```


```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
highTableLong <- round(data.frame(attacks = 0:8, low, m, high, p),3)

highTableBar <- ggplot(highTableLong)+ geom_bar(aes(x=attacks,y=m), stat="identity", fill = "skyblue", alpha= 0.5)+
  geom_errorbar( aes(x=attacks, ymin=low, ymax=high), width=0.4, colour="seashell3", alpha=0.9, size=0.3)+th+xlab("narrow attacks")+ylab("mean activity change")+
  geom_text(aes(x=attacks, y=low-20,
                label=p), size = 2)+labs(title = "Mean impact of narrow attacks on  weekly activity", subtitle = "with 95% confidence intervals and p-values")+
  scale_x_continuous(labels = 0:8, breaks= 0:8)

highTableBar6 <- ggplot(highTableLong[highTableLong$attacks < 6 ,])+ 
  geom_bar(aes(x=attacks,y=m), stat="identity", fill = "skyblue", alpha= 0.5)+
  geom_errorbar( aes(x=attacks, ymin=low, ymax=high), width=0.4, colour="seashell3", alpha=0.9, size=0.3)+th+xlab("narrow attacks")+ylab("mean activity change")+
  geom_text(aes(x=attacks, y=low-20,label=p), size = 2)+
      labs(title = "Mean impact of narrow attacks <6 on  weekly activity",
       subtitle = "with 95% confidence intervals and p-values")+
  scale_x_continuous(labels = 0:5, breaks= 0:5)

highTableBar3 <- ggplot(highTableLong[highTableLong$attacks < 3 ,])+ geom_bar(aes(x=attacks,y=m), stat="identity", fill = "skyblue", alpha= 0.5)+
  geom_errorbar( aes(x=attacks, ymin=low, ymax=high), width=0.4, colour="seashell3", alpha=0.9, size=0.3)+th+xlab("narrow attacks")+ylab("mean activity change")+
  geom_text(aes(x=attacks, y=low-5, label=p), size = 2)+
    labs(title = "Mean impact of narrow attacks <3 on  weekly activity", subtitle = "with 95% confidence intervals and p-values")+scale_x_continuous(labels = 0:2, breaks= 0:2)
```



```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",message= FALSE, warning= FALSE}
attacks <- 0:8
max <- max(attacks)
lowL <- numeric(max+1)
highL <- numeric(max+1)
mL <- numeric(max+1)
pL <- numeric(max+1)
tL <- list()

for(attacks in attacks){
tL[[attacks+1]] <- t.test(data[data$sumLowBefore == attacks,]$activityDiff)

lowL[attacks+1] <- t[[attacks+1]]$conf.int[1]
highL[attacks+1] <- t[[attacks+1]]$conf.int[2]
mL[attacks+1] <- t[[attacks+1]]$estimate
pL[attacks+1] <- t[[attacks+1]]$p.value
}
lowTable<-as.data.frame(round(rbind(0:8,lowL, mL, highL, pL),3))
rownames(lowTable) <- c("attacks","CIlow","estimated m", "CIhigh", "p-value")


attacks <- 0:8
max <- max(attacks)
lowLo <- numeric(max+1)
highLo <- numeric(max+1)
mLo <- numeric(max+1)
pLo <- numeric(max+1)
tLo <- list()

for(attacks in attacks){
tLo[[attacks+1]] <- t.test(data[data$sumLowBefore -
                data$sumHighBefore == attacks,]$activityDiff)

lowLo[attacks+1] <- t[[attacks+1]]$conf.int[1]
highLo[attacks+1] <- t[[attacks+1]]$conf.int[2]
mLo[attacks+1] <- t[[attacks+1]]$estimate
pLo[attacks+1] <- t[[attacks+1]]$p.value
}
lowOnlyTable<-as.data.frame(round(rbind(0:8,lowLo, mLo, highLo, pLo),3))
rownames(lowTable) <- c("attacks","CIlow","estimated m", "CIhigh", "p-value")

```





```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lowTableLong <- round(data.frame(attacks = 0:8, lowL, mL, highL, pL),3)

lowTableBar <- ggplot(lowTableLong)+ geom_bar(aes(x=attacks,y=m), stat="identity", fill = "skyblue", alpha= 0.5)+
  geom_errorbar( aes(x=attacks, ymin=low, ymax=high), width=0.4, colour="seashell3", alpha=0.9, size=0.3)+th+xlab("wide attacks")+ylab("mean activity change")+
  geom_text(aes(x=attacks, y=low-20,
                label=round(p,3)), size = 2)+labs(title = "Mean impact of wide attacks on  weekly activity", subtitle = "with 95% confidence intervals and p-values")+scale_x_continuous(labels = 0:8, breaks= 0:8)


lowOnlyTableLong <- round(data.frame(attacks = 0:8, lowLo, mLo, highLo, pLo),3)

lowOnlyTableBar <- ggplot(lowOnlyTableLong)+ geom_bar(aes(x=attacks,y=m), stat="identity", fill = "skyblue", alpha= 0.5)+
  geom_errorbar( aes(x=attacks, ymin=low, ymax=high), width=0.4, colour="seashell3", alpha=0.9, size=0.3)+th+xlab("wide only attacks")+ylab("mean activity change")+
  geom_text(aes(x=attacks, y=low-20,
                label=round(p,3)), size = 2)+labs(title = "Mean impact of wide only attacks on  weekly activity", subtitle = "with 95% confidence intervals and p-values")+scale_x_continuous(labels = 0:8, breaks= 0:8)
```




T-test based estimates for activity change divided by numbers of narrow attacks received:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
highTable
```



T-test based estimates for activity change divided by numbers of wide attacks received:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lowTable
```


T-test based estimates for activity change divided by numbers of wide only attacks  received:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lowOnlyTable
```




```{r highTableBar,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
highTableBar
```

```{r highTableBar6,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
highTableBar6
```


```{r highTableBar3,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
highTableBar3
```

```{r lowTableBar,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowTableBar
```


```{r lowOnlyTableBar,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
lowOnlyTableBar
```






```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
h6 <- data[data$sumHighBefore == 6,]
h7 <- data[data$sumHighBefore == 7,]
h8 <- data[data$sumHighBefore == 8,]

# power for 6 attacks
a <- mean(data$activityDiff)
s <- sd(h7$activityDiff)
n <- 8
error <- qt(0.975,df=n-1)*s/sqrt(n)
left <- a-error
right <- a+error
assumed <- a - 80
tleft <- (left-assumed)/(s/sqrt(n))
tright <- (right-assumed)/(s/sqrt(n))
p <- pt(tright,df=n-1)-pt(tleft,df=n-1)
power6 <- 1-p
power6

# power for 7 attacks
a <- mean(data$activityDiff)
s <- sd(h7$activityDiff)
n <- 8
error <- qt(0.975,df=n-1)*s/sqrt(n)
left <- a-error
right <- a+error
assumed <- a - 80
tleft <- (left-assumed)/(s/sqrt(n))
tright <- (right-assumed)/(s/sqrt(n))
p <- pt(tright,df=n-1)-pt(tleft,df=n-1)
power7 <-1-p
power7

# power for 8 attacks
a <- mean(data$activityDiff)
s <- sd(h8$activityDiff)
n <- 4
error <- qt(0.975,df=n-1)*s/sqrt(n)
left <- a-error
right <- a+error
assumed <- a - 80
tleft <- (left-assumed)/(s/sqrt(n))
tright <- (right-assumed)/(s/sqrt(n))
p <- pt(tright,df=n-1)-pt(tleft,df=n-1)
power8 <- 1-p
power8
```

robabilities that this effect would be discovered by a single sample t-test for 6, 7, and 8 attacks are `r round(c(power6,power7,power8),3)`, and so tests for higher numbers of attacks are underpowered. 


We run single t-tests on different groups to estimate different means and we don't use t-test for hypothesis testing. To alleviate concerns about multiple testing and increased risk of type I error,  we also  performed an  ANOVA tests, which strongly suggest non-random correlation between the numbers of attacks and activity change. 
 Furthermore,  80 comparison rows in Tukey's Honest Significance Test [@Tukey1949] have conservatively adjusted p-value below 0.05.  
 
 
 
 
```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",message=FALSE,warning=FALSE}
highAnova <- aov(activityDiff ~ as.factor(sumHighBefore), data = data)
lowAnova <- aov(activityDiff ~ as.factor(sumLowBefore), data = data)
lowOnlyAnova <- aov(activityDiff ~ as.factor(sumLowBefore-sumHighBefore), data = data)

library(descr,quietly=TRUE)
library(pander,quietly=TRUE)
library(papeR,quietly=TRUE)
sh <- xtable(summary(highAnova))
rownames(sh) <- c("narrow","residuals")
sh 
```


```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
sl <- xtable(summary(lowAnova))
rownames(sl) <- c("narrow","residuals")
sl
```



```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
so <- xtable(summary(lowOnlyAnova))
rownames(so) <- c("narrow","residuals")
so 
```



```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",fig.margin=TRUE}
means <- numeric(10000)
for(run in 1:10000){
means[run] <- mean(sample(data[data$sumHighBefore==2,]$activityDiff,30))
}
distr <- ggplot(data[data$sumHighBefore==2,],aes(x = activityDiff)) + geom_histogram(bins = 100)+th+
  ggtitle("Distribution of activityDiff for narrow attacks before = 2")


sampDistr <- ggplot() + geom_histogram(aes(x = means),bins = 100)+th+ggtitle("Simulated sampling distribution for the same  with n=30 and 10 000 runs")
```


```{r distr,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE,  dpi = 300, message = FALSE, warning = FALSE, fig.width= 5}
distr
```


```{r sampDistr,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE,  dpi = 300, message = FALSE, warning = FALSE, fig.width= 6}
sampDistr
```


- $p$-values and confidence intervals are sensitive to undesirable factors, such as stopping intention in experiment design [@Kruschke2015].



```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",warning=FALSE, message=FALSE}
library(BEST)
priorsWide <- list(muM = 0, muSD = 50)
priorsInformed <- list(muM = -1.11, muSD = 44.47)
priorsFit <- list(muM = -1.11, muSD = 7.5)
```


```{r priorWide,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, message = FALSE, warning = FALSE}
priorWide <- ggplot(data = data.frame(x = c(-200, 200)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 50)) + ylab("") +
  scale_y_continuous(breaks = NULL)+th+xlab("expected activity change")+labs(title = "Wide prior", subtitle ="Normal prior with m = 0, sd = 50")
priorWide
```


